{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccac457e",
   "metadata": {},
   "source": [
    "### Retriever API Usage\n",
    "\n",
    "This notebook showcases how to use the NVIDIA RAG retriever APIs to fetch relevant document passages based on user queries and also generate responses using end-to-end RAG APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cfe48-22f4-4526-91f3-b555c45b406d",
   "metadata": {},
   "source": [
    "\n",
    "- Ensure the rag-server container is running before executing the notebook by following the steps in [Get Started](../docs/deploy-docker-self-hosted.md).\n",
    "- Please run the [ingestion notebook](./ingestion_api_usage.ipynb) as a prerequisite to using this notebook.\n",
    "- Replace `IP_ADDRESS` with the actual server URL if the API is hosted on another system.\n",
    "\n",
    "You can now execute each cell in sequence to test the API.\n",
    "#### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28c582-c0ac-4f04-8c97-433e79be8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp\n",
    "import json\n",
    "import os\n",
    "\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541f87b-c9b7-46dc-8d1e-7fb6eb885374",
   "metadata": {},
   "source": [
    "#### 2. Setup Base Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcab5bd-60d5-4354-a3f7-bf8039d78389",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = (\n",
    "    \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\"\n",
    ")  # Replace this with the correct IP address\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451651b-23b6-4854-a734-656c3fac253f",
   "metadata": {},
   "source": [
    "#### 3. Health Check Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint performs a health check on the server. It returns a 200 status code if the server is operational. It also returns the status of the dependent services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753b52b-2728-4201-bf0e-0fbbf4d115bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_health_status():\n",
    "    \"\"\"Fetch health status asynchronously.\"\"\"\n",
    "    url = f\"{BASE_URL}/v1/health\"\n",
    "    params = {\"check_dependencies\": \"True\"}  # Check health of dependencies as well\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            await print_response(response)\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "await fetch_health_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf4e4b-78cd-4a6f-a97a-971c18dafcaf",
   "metadata": {},
   "source": [
    "#### 4. Generate Answer Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint generates a streaming AI response to a given user message. The system message is specified in the [prompts.yaml]((../src/nvidia_rag/rag_server/prompt.yaml)) file. This API retrieves the relevant chunks related to the query from knowledge base, adds them as part of the LLM prompt and returns a streaming response. It supports parameters like temperature, top_p, knowledge base usage, and also generates based on the specified vector collection. \n",
    "\n",
    "The API endpoint also returns multimodal base64 encoded data if the cited source is an image as part of the returned document chunks. The citations field is always populated as part of the first chunk returned in the streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fdcfd-c845-4cbf-b1ae-d53b2d631b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/v1/generate\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How does the price of bluetooth speaker compare with hammer?\",\n",
    "        }\n",
    "    ],\n",
    "    \"use_knowledge_base\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"reranker_top_k\": 2,\n",
    "    \"vdb_top_k\": 10,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_names\": [\"multimodal_data\"],\n",
    "    \"enable_query_rewriting\": True,\n",
    "    \"enable_reranker\": True,\n",
    "    \"enable_citations\": True,\n",
    "    \"stop\": [],\n",
    "    \"filter_expr\": \"\",\n",
    "    # Override model endpoints and details if needed\n",
    "    #\"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "    #\"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    #\"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    #\"llm_endpoint\": \"\",\n",
    "    #\"embedding_endpoint\": \"\",\n",
    "    #\"reranker_endpoint\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ce167-ccb2-434d-8468-1beb4ca265eb",
   "metadata": {},
   "source": [
    "#### 5. Parse RAG Metrics from the last chunk of the streaming response\n",
    "**Purpose:**\n",
    "This cell is an extension of the generate endpoint call, where the streaming response from the endpoint is parsed in chunks and the response content as well as RAG metrics are extraced from these chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fa3c3-7f9c-4920-87aa-4871cc07af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_metrics_from_response(response):\n",
    "    buffer = \"\"\n",
    "    async for chunk in response.content.iter_chunked(8192):\n",
    "        if chunk:\n",
    "            # Decode the chunk and add to buffer\n",
    "            chunk_str = chunk.decode(\"utf-8\")\n",
    "            buffer += chunk_str\n",
    "\n",
    "            # Process complete lines in the buffer\n",
    "            lines = buffer.split('\\n')\n",
    "            buffer = lines[-1]  # Keep the incomplete last line in buffer\n",
    "\n",
    "            for line in lines[:-1]:  # Process all complete lines\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"data: \"):\n",
    "                    json_str = line[6:]  # Remove \"data: \" prefix\n",
    "                    if json_str:  # Skip empty data lines\n",
    "                        try:\n",
    "                            data = json.loads(json_str)\n",
    "\n",
    "                            # Process the message content\n",
    "                            message = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                            if message:\n",
    "                                print(message, end=\"\")  # Stream the message content\n",
    "\n",
    "                            # Check if it's the last chunk based on the \"finish_reason\"\n",
    "                            finish_reason = data.get(\"choices\", [{}])[0].get(\"finish_reason\")\n",
    "                            if finish_reason == \"stop\":\n",
    "                                print(\"\\nMetrics:\")\n",
    "                                metrics = data.get(\"metrics\", {})\n",
    "                                print(f\"LLM Generation Time: {metrics.get('llm_generation_time_ms', 'N/A')} ms\")\n",
    "                                print(f\"LLM TTFT Time: {metrics.get('llm_ttft_ms', 'N/A')} ms\")\n",
    "                                print(f\"Context Reranker Time: {metrics.get('context_reranker_time_ms', 'N/A')} ms\")\n",
    "                                print(f\"Retrieval Time: {metrics.get('retrieval_time_ms', 'N/A')} ms\")\n",
    "                                print(f\"RAG TTFT Time: {metrics.get('rag_ttft_ms', 'N/A')} ms\")\n",
    "                                return\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            # Skip malformed JSON chunks\n",
    "                            continue\n",
    "\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await parse_metrics_from_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Call the async function\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc614b-846f-4a91-a09a-061d8e66e21c",
   "metadata": {},
   "source": [
    "#### 6. Document Search Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint searches for the most relevant documents in the vector store based on a query. You can specify the maximum number of documents to retrieve using `reranker_top_k`.  \n",
    "\n",
    "The `content` of the document is returned as well, in case of images representing charts or table, in a base64 represention. Developers can use these base64 representations for rendering multimodal citations to end users. The textual representation of this content is available under `description` field of `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e2709-9de9-49ca-aa43-66182fe31d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/v1/search\"\n",
    "payload = {\n",
    "    \"query\": \"Tell me about robert frost's poems\",\n",
    "    \"reranker_top_k\": 2,\n",
    "    \"vdb_top_k\": 10,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_names\": [\n",
    "        \"multimodal_data\"\n",
    "    ],  # Multiple collection retrieval can be used by passing multiple collection names\n",
    "    \"messages\": [],\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": True,\n",
    "    # Override model endpoints and details if needed\n",
    "    #\"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    #\"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    #\"embedding_endpoint\": \"\",\n",
    "    #\"reranker_endpoint\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "async def document_seach(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await document_seach(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f4a3b-cd1d-4923-9d4c-a59460d12572",
   "metadata": {},
   "source": [
    "#### 7. [Optional] Document Search Endpoint with metadata filtering\n",
    "\n",
    "**Purpose:** Filtering can be performed with custom-metadata provided during ingestion. Similarly `filter_expr` field can be passed in `/generate` endpoint to filter the retrieved chunks from the RAG. \n",
    "\n",
    "Before using custom-metadata filtering, kindly ensure the custom metadata is added at ingestion stage. The filtering can be performed using Milvus filtering expression (Reference: [Milvus Filtering](https://milvus.io/docs/boolean.md)). An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3cdc4-d28d-47ff-8528-6019a809159e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = f\"{BASE_URL}/v1/search\"\n",
    "payload = {\n",
    "    \"query\": \"What is lion doing?\",\n",
    "    \"reranker_top_k\": 10,\n",
    "    \"vdb_top_k\": 100,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_names\": [\n",
    "        \"multimodal_data\"\n",
    "    ],  # Multiple collection retrieval can be used by passing multiple collection names\n",
    "    \"messages\": [],\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": True,\n",
    "    \"filter_expr\": 'content_metadata[\"meta_field_1\"] == \"multimodal document\"',  # Following is an example filter expression\n",
    "}\n",
    "\n",
    "\n",
    "async def document_seach(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await document_seach(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fcb7b-36c1-4f3b-9321-336b4d538f58",
   "metadata": {},
   "source": [
    "#### 8. [Optional] Retrieve documents summary\n",
    "You can execute this cell if summary generation was enabled during document upload using `generate_summary: bool` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086a95f-2d60-4526-b333-b40122e7f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_summary():\n",
    "    url = f\"{BASE_URL}/v1/summary\"\n",
    "    params = {\n",
    "        \"collection_name\": \"multimodal_data\",\n",
    "        \"file_name\": \"woods_frost.pdf\",\n",
    "        \"blocking\": \"false\",\n",
    "        \"timeout\": 20,\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, params=params) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await fetch_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
