{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1de2541",
   "metadata": {},
   "source": [
    "# Evaluating RAG Pipeline: Answer Accuracy, Context Relevancy, and Groundedness via RAGAS\n",
    "\n",
    "In this notebook, we will evaluate our RAG system using three key metrics with the [Ragas](https://docs.ragas.io/en/stable/) library. \n",
    "\n",
    "Ragas provides a set of evaluation metrics that can be used to measure the performance of your LLM application. These metrics are designed to help you objectively measure the performance of your application. \n",
    "## Evaluation Metrics\n",
    "\n",
    "In this notebook, we will use the following three metrics, introduced to Ragas by NVIDIA:\n",
    "1. **Answer Accuracy**: Measures the agreement between a model’s response and a reference ground truth for a given question.\n",
    "2. **Context Relevancy**: Evaluates whether the retrieved contexts (chunks or passages) are pertinent to the user input. \n",
    "3. **Response Groundedness**: Measures how well a response is supported or \"grounded\" by the retrieved contexts. It assesses whether each claim in the response can be found, either wholly or partially, in the provided contexts.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes you are familiar with the RAG system and you have both `rag-server` and `ingestor-server` up and running. If you have not done that, you can refer to [Get Started](../docs/deploy-docker-self-hosted.md) to start the RAG server.\n",
    "\n",
    "## 1. Download Evaluation Documents\n",
    "\n",
    "First, let's download the FinanceBench dataset to evaluate our RAG system. This dataset includes PDF files with information and reports about publicly traded companies, as well as ground truth question and answer pairs.\n",
    "\n",
    "We'll clone the repository into our data directory in a subdirectory called `financebench`. The PDFs can be found in the `pdfs` subdirectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/patronus-ai/financebench.git ../data/financebench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a5f07",
   "metadata": {},
   "source": [
    "## 2. Ingest Evaluation Documents\n",
    "\n",
    "For evaluation, we will use the FinanceBench dataset. In the data directory, we have the PDF files for the FinanceBench dataset, as well as the `financebench_open_source.jsonl` file, which includes ground truth question and answer pairs. \n",
    "\n",
    "Let's start by creating a collection called `financebench` and upload the relevant documents.\n",
    "\n",
    "This process is similar to the `ingestion_api_usage` notebook. First, we'll install the required packages and set up our API connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required Python packages\n",
    "! pip install aiohttp langchain-nvidia-ai-endpoints ragas httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"ingestor-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\" # Replace this with the correct IP address\n",
    "INGESTOR_SERVER_PORT = \"8082\"\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{INGESTOR_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_collection(\n",
    "    collection_name: str = None,\n",
    "    embedding_dimension: int = 2048,\n",
    "    metadata_schema: list = []\n",
    "):\n",
    "    \"\"\"Create a new collection in the vector database.\"\"\"\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_dimension\": embedding_dimension,\n",
    "        \"metadata_schema\": metadata_schema\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/collection\", json=data, headers=HEADERS) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "# Create the financebench collection\n",
    "await create_collection(\n",
    "    collection_name=\"financebench\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92418e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all PDF files from the financebench directory\n",
    "FILEPATHS = glob.glob(os.path.join(\"../data/financebench/pdfs\", \"*.pdf\"))\n",
    "\n",
    "async def upload_documents(collection_name: str = \"\"):\n",
    "    \"\"\"Upload documents to the specified collection.\"\"\"\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"blocking\": False,  # If True, upload is blocking; else async. Status API not needed when blocking\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 150\n",
    "        },\n",
    "        \"generate_summary\": False  # Set to True to optionally generate summaries for all documents after ingestion\n",
    "    }\n",
    "\n",
    "    form_data = aiohttp.FormData()\n",
    "    \n",
    "    # Add all PDF files to the form data\n",
    "    for file_path in FILEPATHS:\n",
    "        form_data.add_field(\"documents\", open(file_path, \"rb\"), filename=os.path.basename(file_path), content_type=\"application/pdf\")\n",
    "\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data) as response: # Replace with session.patch for reingesting\n",
    "                await print_response(response)\n",
    "                # Return the response JSON for task_id extraction\n",
    "                response_json = await response.json()\n",
    "                return response_json\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            return None\n",
    "\n",
    "# Store the response and extract task_id\n",
    "upload_response = await upload_documents(collection_name=\"financebench\")\n",
    "task_id = upload_response.get(\"task_id\") if upload_response else None\n",
    "print(f\"Extracted task_id: {task_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b7771",
   "metadata": {},
   "source": [
    "**⚠️ Note**: During the document ingestion process, two files (`INTEL_2023_8K_dated-2023-08-16.pdf` and `INTEL_2023_8K_dated-2023-02-10.pdf`) may fail to process due to formatting issues. This is expected and can be safely ignored, as it will not affect the evaluation methodology or results. The remaining documents in the dataset are sufficient for comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a few minutes to complete depending on the number of documents uploaded\n",
    "async def get_task_status(\n",
    "    task_id: str\n",
    "):\n",
    "\n",
    "    params = {\n",
    "        \"task_id\": task_id,\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(f\"{INGESTOR_BASE_URL}/v1/status\", params=params, headers=HEADERS) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "# Use the extracted task_id from the upload_documents response\n",
    "if task_id:\n",
    "    await get_task_status(task_id=task_id)\n",
    "else:\n",
    "    print(\"No task_id available. Please run the upload_documents cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5edff",
   "metadata": {},
   "source": [
    "## 3. Create Dataset for Ragas Evaluation\n",
    "\n",
    "In `data/financebench/data`, there is a file called `financebench_open_source.jsonl`. This file contains questions about the PDFs, as well as corresponding ground truth answers.\n",
    "\n",
    "For each ground-truth question and answer pair, we will:\n",
    "1. Generate an answer from our RAG system\n",
    "2. Retrieve the relevant document contexts\n",
    "3. Create a dataset suitable for Ragas evaluation\n",
    "\n",
    "The answer and context retrieval from the RAG system is similar to the `retriever_api_usage` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\" #Replace this with the correct IP address\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "generate_url = f\"{RAG_BASE_URL}/v1/generate\"\n",
    "\n",
    "async def generate_answer(payload):\n",
    "    \"\"\"Generate an answer using the RAG server.\"\"\"\n",
    "    rag_response = \"\"\n",
    "    citations = []\n",
    "    is_first_token = True\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "        try:\n",
    "            async with client.stream(\"POST\", url=generate_url, json=payload) as response:\n",
    "                # Raise an exception for bad status codes like 4xx or 5xx\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # iterate over the response lines\n",
    "                async for line in response.aiter_lines():\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        json_str = line[6:].strip()\n",
    "                        if not json_str:\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            data = json.loads(json_str)\n",
    "\n",
    "                            # --- Extract the response from the RAG server ---\n",
    "                            message = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                            if message:\n",
    "                                rag_response += message\n",
    "\n",
    "                            # --- Extract the citations from the RAG server ---\n",
    "                            if is_first_token and data.get(\"citations\"):\n",
    "                                for result in data.get(\"citations\", {}).get(\"results\", []):\n",
    "                                    description = result.get(\"metadata\", {}).get(\"description\")\n",
    "                                    if description:\n",
    "                                        citations.append(description)\n",
    "                                is_first_token = False\n",
    "\n",
    "                            finish_reason = data.get(\"choices\", [{}])[0].get(\"finish_reason\")\n",
    "                            if finish_reason == \"stop\":\n",
    "                                return rag_response, citations\n",
    "\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Skipping malformed JSON line: {json_str}\")\n",
    "                            continue\n",
    "        \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"An error occurred while requesting {e.request.url!r}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return rag_response, citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question and ground-truth answer pairs from the FinanceBench dataset\n",
    "with open('../data/financebench/data/financebench_open_source.jsonl', 'r') as file:\n",
    "    gt_qa_pairs = [json.loads(line) for line in file]\n",
    "\n",
    "print(f\"Loaded {len(gt_qa_pairs)} question-answer pairs from FinanceBench dataset\")\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# For the purposes of keeping this demo brief, we will only evaluate on 50 questions. \n",
    "# You can increase this to the full dataset for more comprehensive results.\n",
    "n = 50 \n",
    "print(f\"Evaluating on {n} questions...\")\n",
    "\n",
    "for idx, qa_pair in enumerate(gt_qa_pairs[:n]):\n",
    "    question = qa_pair['question']\n",
    "    \n",
    "    print(f\"Processing question {idx + 1}/{n}: {question[:100]}...\")\n",
    "\n",
    "    generate_payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        \"use_knowledge_base\": True,\n",
    "        \"reranker_top_k\": 2,\n",
    "        \"vdb_top_k\": 10,\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        \"collection_names\": [\"financebench\"],\n",
    "        \"enable_reranker\": True,\n",
    "        \"enable_citations\": True,\n",
    "        \"stop\": [],\n",
    "        \"filter_expr\": ''\n",
    "    }\n",
    "    \n",
    "    rag_answer, citations = await generate_answer(generate_payload)\n",
    "\n",
    "    dataset.append({\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": citations,\n",
    "        \"response\": rag_answer,\n",
    "        \"reference\": qa_pair['answer'],\n",
    "    })\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} entries for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68742",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Evaluate with Ragas\n",
    "\n",
    "In this example, we will use the NVIDIA hosted endpoint for our judge model. To use this endpoint, please provide your NVIDIA API Key below. \n",
    "\n",
    "### Rate Limiting Considerations\n",
    "\n",
    "When using the public endpoint for the Judge LLM, you will likely encounter rate limit errors. We can try to reduce the number of errors by adjusting the configuration, which we do below. \n",
    "\n",
    "Alternatively, you can use self-hosted NIM Microservices endpoints to avoid these errors altogether. If you're using a self-hosted NIM, you do not need to provide your API Key.\n",
    "\n",
    "### Getting Your NVIDIA API Key\n",
    "\n",
    "To generate an API Key:\n",
    "1. Go to [build.nvidia.com](https://build.nvidia.com/)\n",
    "2. Click the green \"Get API Key\" button in the top right corner\n",
    "3. Paste your key below to save it as an environment variable\n",
    "\n",
    "### Self-Hosted Option\n",
    "\n",
    "To deploy the Judge LLM as a NIM on your own infrastructure, follow the instructions [here](https://build.nvidia.com/mistralai/mixtral-8x22b-instruct/deploy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: Models on build.nvidia.com are rate limited.\n",
    "# To avoid rate-limit issues, either deploy the judge model locally (self-hosted NIM)\n",
    "# or use any OpenAI-compatible LLM as the judge for evaluation.\n",
    "from langchain_nvidia_ai_endpoints.chat_models import ChatNVIDIA\n",
    "\n",
    "# Initialize the judge LLM for evaluation\n",
    "# You can use any other model by creating Chat Model object\n",
    "llm = ChatNVIDIA(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation dataset from our collected data\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "print(f\"Created evaluation dataset with {len(evaluation_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metrics and evaluation components\n",
    "from ragas.metrics import AnswerAccuracy, ContextRelevance, ResponseGroundedness\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Wrap the LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "\n",
    "custom_config = RunConfig(max_workers=1, max_wait=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3571af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation with our three metrics\n",
    "print(\"Starting Ragas evaluation...\")\n",
    "print(\"This may take several minutes depending on the dataset size.\")\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[AnswerAccuracy(), ContextRelevance(), ResponseGroundedness()],\n",
    "    llm=evaluator_llm, \n",
    "    run_config=custom_config\n",
    ")\n",
    "\n",
    "print(\"Evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9dde6",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "Finally, let's examine our evaluation results. We'll look at both the overall metrics and individual sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to pandas DataFrame for detailed analysis of individual queries\n",
    "results_df = results.to_pandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Set the option to display ALL columns, preventing the '...'\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 2. To prevent long text in cells from being cut off, you can set the column width\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
