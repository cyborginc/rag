{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1de2541",
   "metadata": {},
   "source": [
    "# Evaluating RAG Pipeline- Answer Accuracy, Context Relevancy, and Groundedness via RAGAS\n",
    "\n",
    "In this notebook, we will evaluate our RAG system for three metrics using the [Ragas](https://docs.ragas.io/en/stable/) library. \n",
    "\n",
    "Ragas provides a set of evaluation metrics that can be used to measure the performance of your LLM application. These metrics are designed to help you objectively measure the performance of your application. \n",
    "\n",
    "In this notebook, we will use the following three metrics, introduced to Ragas by NVIDIA.\n",
    "1. Answer Accuracy measures the agreement between a modelâ€™s response and a reference ground truth for a given question.\n",
    "2. Context Relevancy Context Relevance evaluates whether the retrieved_contexts (chunks or passages) are pertinent to the user_input. \n",
    "3. Response Groundedness measures how well a response is supported or \"grounded\" by the retrieved contexts. It assesses whether each claim in the response can be found, either wholly or partially, in the provided contexts.\n",
    "\n",
    "## 1. Download Evaluation Documents\n",
    "\n",
    "First, let's download a dataset to evaluate our RAG system on. We will use the FinanceBench dataset, which includes PDF files with information and reports about publicly traded companies, as well as ground truth question and answer pairs.\n",
    "\n",
    "Let's start by cloning the repo into our data directory, in a subdirectory called `financebench`. Inside `financebench`, you can find the PDFs in a subdirectory called `pdfs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/patronus-ai/financebench.git ../data/financebench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a5f07",
   "metadata": {},
   "source": [
    "## 2. Ingest Evaluation Documents\n",
    "\n",
    "For evaluation, we will use the KG_RAG dataset. In the data directory, we have the pdf files for the KG_RAG dataset, as well as train.json file, which includes ground truth question and answer pairs. \n",
    "Let's start by creating a collection called `financebench`, and upload the relevant documents.\n",
    "\n",
    "This is similar to the `ingestion_api_usage` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import os\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"ingestor-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\" # Replace this with the correct IP address\n",
    "INGESTOR_SERVER_PORT = \"8082\"\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{INGESTOR_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_collection(\n",
    "    collection_name: list = None,\n",
    "    embedding_dimension: int = 2048,\n",
    "    metadata_schema: list = []\n",
    "):\n",
    "\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"embedding_dimension\": embedding_dimension,\n",
    "        \"metadata_schema\": metadata_schema\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/collection\", json=data, headers=HEADERS) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "# Call create collection method\n",
    "await create_collection(\n",
    "    collection_name=\"financebench\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92418e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATHS = glob.glob(os.path.join(\"../data/financebench/pdfs\", \"*.pdf\"))\n",
    "\n",
    "async def upload_documents(collection_name: str = \"\"):\n",
    "\n",
    "    data = {\n",
    "        \"collection_name\": collection_name,\n",
    "        \"blocking\": False, # If True, upload is blocking; else async. Status API not needed when blocking\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 150\n",
    "        },\n",
    "        \"generate_summary\": False # Set to True to optionally generate summaries for all documents after ingestion\n",
    "    }\n",
    "\n",
    "    form_data = aiohttp.FormData()\n",
    "    for file_path in FILEPATHS:\n",
    "        form_data.add_field(\"documents\", open(file_path, \"rb\"), filename=os.path.basename(file_path), content_type=\"application/pdf\")\n",
    "\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data) as response: # Replace with session.patch for reingesting\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "await upload_documents(collection_name=\"financebench\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a few minutes to complete depending on the number of documents uploaded\n",
    "async def get_task_status(\n",
    "    task_id: str\n",
    "):\n",
    "\n",
    "    params = {\n",
    "        \"task_id\": task_id,\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(f\"{INGESTOR_BASE_URL}/v1/status\", params=params, headers=HEADERS) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "await get_task_status(task_id=[\"*****************************\"]) # Please enter the task_id obtained from upload documents API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5edff",
   "metadata": {},
   "source": [
    "## 3. Create Dataset for Ragas Evaluation\n",
    "\n",
    "In `data/financebench/data`, there is a file called `financebench_open_source.jsonl`. This file contains questions about the PDFs, as well as correlating ground truth answers.\n",
    "For each ground-truth question and answer`, we will generate an answer from our RAG system and retrieve the relevant docs. \n",
    "\n",
    "The answer and context retrieval from the RAG system is similar to `retriever_api_uasge` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ade53",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPADDRESS = \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\" #Replace this with the correct IP address\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "async def print_response(response):\n",
    "    \"\"\"Helper to print API response.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genrate_url = f\"{RAG_BASE_URL}/v1/generate\"\n",
    "async def generate_answer(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=genrate_url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "search_url = f\"{RAG_BASE_URL}/v1/search\"\n",
    "async def document_seach(payload):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=search_url, json=payload) as response:\n",
    "                await print_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we open get the question and ground-truth answer pairs. This is provided for us us a part of the KG RAG dataset. \n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('../data/financebench/data/financebench_open_source.jsonl', 'r') as file:\n",
    "    gt_qa_pairs = [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "dataset = []\n",
    "\n",
    "n = 50 # For the purposes of keeping this demo brief, we will only evaluate on 50 questions. You can increase this to the full dataset of 194 questions for more comprehensive results.\n",
    "for i in gt_qa_pairs[:n]:\n",
    "    question = i['question']\n",
    "\n",
    "    print(question)\n",
    "\n",
    "    generate_payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        \"use_knowledge_base\": True,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.7,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"reranker_top_k\": 2,\n",
    "        \"vdb_top_k\": 10,\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        \"collection_names\": [\"financebench\"],\n",
    "        \"enable_query_rewriting\": True,\n",
    "        \"enable_reranker\": True,\n",
    "        \"enable_citations\": True,\n",
    "        \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "        \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "        \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "        # Provide url of the model endpoints if deployed elsewhere\n",
    "        # \"llm_endpoint\": \"\",\n",
    "        #\"embedding_endpoint\": \"\",\n",
    "        #\"reranker_endpoint\": \"\",\n",
    "        \"stop\": [],\n",
    "        \"filter_expr\": ''\n",
    "        }\n",
    "    \n",
    "    search_payload={\n",
    "        \"query\": question,\n",
    "        \"reranker_top_k\": 2,\n",
    "        \"vdb_top_k\": 10,\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        \"collection_names\": [\"financebench\"],\n",
    "        \"messages\": [],\n",
    "        \"enable_query_rewriting\": True,\n",
    "        \"enable_reranker\": True,\n",
    "        \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "        # Provide url of the model endpoints if deployed elsewhere\n",
    "        #\"embedding_endpoint\": \"\",\n",
    "        #\"reranker_endpoint\": \"\",\n",
    "        \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "\n",
    "        }\n",
    "    \n",
    "    rag_answer = await generate_answer(generate_payload)\n",
    "    rag_search = await document_seach(search_payload)\n",
    "\n",
    "    dataset.append({\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": rag_search,\n",
    "        \"response\": rag_answer,\n",
    "        \"reference\": i['answer'],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68742",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Evaluate with Ragas\n",
    "\n",
    "In this example, we will use the NVIDIA hosted enpoint for our judge model. To use this endpoint, please provide your NVIDIA API Key below. \n",
    "\n",
    "When using the public endpoint for the Judge LLM, you will likely encounter Rate Limit Error. We can try to reduce the number of errors by adjusting the config, which we do below. \n",
    "\n",
    "Alternatively, you can use self hosted NIM Microservices endpoints to avoid these errors altogether. If you're using a self-hosted NIM you do not need to provide you API Key. Paste your key below to save it as an environment variable. \n",
    "                                              \n",
    "To generate an API Key, go to [build.nvidia.com](https://build.nvidia.com/), and click the green \"Get API Key\" button in the top right corner.\n",
    "\n",
    "To deploy the Judge LLM as a NIM on your own, follow the instructions [here](https://build.nvidia.com/mistralai/mixtral-8x22b-instruct/deploy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints.chat_models import ChatNVIDIA\n",
    "\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-***\"\n",
    "\n",
    "llm = ChatNVIDIA(model=\"nvidia/llama-3.3-nemotron-super-49b-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator_llm = LangchainLLMWrapper(llm)\n",
    "from ragas.metrics import AnswerAccuracy, ContextRelevance, ResponseGroundedness\n",
    "from ragas import evaluate\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "evaluator_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "\n",
    "custom_config = RunConfig(max_workers=1, max_wait=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3571af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(dataset=evaluation_dataset,metrics=[AnswerAccuracy(), ContextRelevance(), ResponseGroundedness()],llm=evaluator_llm, run_config=custom_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9dde6",
   "metadata": {},
   "source": [
    "Finally, let's take a look at our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
