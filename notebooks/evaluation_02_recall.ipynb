{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cddef13",
   "metadata": {},
   "source": [
    "# Evaluating RAG Pipeline - Recall\n",
    "\n",
    "In this notebook, we will calculate a metric called **recall** at multiple top-k levels. Here, `k` is the number of contexts retrieved from our RAG system, and the recall metric evaluates how well the retrieval system performs by measuring the proportion of relevant documents that are successfully retrieved at different top-k cutoffs (1, 3, 5, 10).\n",
    "\n",
    "By default, we will set `k=10` in this notebook to calculate Recall@1, Recall@3, Recall@5, and Recall@10. You can modify the `top_k` variable to retrieve more or fewer documents as needed.\n",
    "\n",
    "**Prerequisites**: This notebook assumes you have already completed [`evaluation_01_ragas`](./evaluation_01_ragas.ipynb), your `rag-server` and `ingestor-server` are running, and the financebench data has been ingested. Since the evaluation data was processed in the previous notebook, we can directly use the retrieval endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required Python packages\n",
    "! pip install pandas tqdm requests\n",
    "\n",
    "# Installing required Python packages for gt_page_mapper.py\n",
    "! pip install pypdf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158655c",
   "metadata": {},
   "source": [
    "## 1. Map Contexts to Documents\n",
    "\n",
    "### Prerequisites\n",
    "Before proceeding with this notebook, ensure that you have completed the following:\n",
    "\n",
    "1. **Completed [`evaluation_01_ragas`](./evaluation_01_ragas.ipynb) notebook**: This notebook downloads the `financebench` dataset.\n",
    "2. **RAG server is running**: `rag-server` should be up and running.\n",
    "3. **Data is ingested**: The financebench PDF documents should be ingested into your RAG system with collection name `financebench`.\n",
    "\n",
    "### Ground Truth Context Mapping\n",
    "\n",
    "In this notebook, we will use the `financebench` dataset to evaluate recall metrics. We'll use the `gt_page_mapper.py` script to map ground truth contexts to their corresponding document pages in the dataset. \n",
    "\n",
    "The script processes various document formats (PDF, TXT) and determines the most relevant pages using either explicit page numbers or text similarity. In our case, the financebench dataset contains only PDFs.\n",
    "\n",
    "**Note**: If you are using a different dataset, you will likely need to update the `gt_page_mapper.py` script according to your ground truth data formatting.\n",
    "\n",
    "The script will generate a new JSON file (`gt_file_pages-financebench.json`) that contains the ground truth context metadata (document name and page number) for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 gt_page_mapper.py --dataset financebench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73d9bd",
   "metadata": {},
   "source": [
    "## 2. Define Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data\n",
    "DATASET_BASE_DIR =\"../data/financebench\"\n",
    "GT_FILENAME_PAGE_DATA = \"./gt_file_pages-financebench.json\"\n",
    "EVAL_DATA = \"data/financebench_open_source.jsonl\"\n",
    "\n",
    "# endpoints\n",
    "IPADDRESS = \"rag-server\" if os.environ.get(\"AI_WORKBENCH\", \"false\") == \"true\" else \"localhost\" #Replace this with the correct IP address\n",
    "RAG_SERVER_PORT = \"8081\"\n",
    "BASE_URL = f\"http://{IPADDRESS}:{RAG_SERVER_PORT}\"  # Replace with your server URL\n",
    "\n",
    "collection_name = \"financebench\"\n",
    "dataset_name = \"financebench\"\n",
    "\n",
    "error_count = 0\n",
    "top_k = 10 # change this to retrieve more/less documents\n",
    "\n",
    "TIMEOUT = 180\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523fad4",
   "metadata": {},
   "source": [
    "## 3. Define all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_data(dataset_path):\n",
    "\n",
    "    data =[]\n",
    "\n",
    "    # Open and load the JSONL file\n",
    "    with open(dataset_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line) # Load JSON data into a Python dictionary\n",
    "            filtered_entry = {\n",
    "                \"id\": entry[\"financebench_id\"],\n",
    "                \"question\": entry[\"question\"],\n",
    "                \"answer\": entry[\"answer\"],\n",
    "                \"context\": entry[\"evidence\"],\n",
    "            }\n",
    "            data.append(filtered_entry)\n",
    "\n",
    "    \n",
    "    print(\"    - Loaded Evaluation data\")\n",
    "    print(\"    - Number of data points\", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd583194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_documents(query, BASE_URL, collection_name, top_k):\n",
    "    data_search = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k,\n",
    "        \"collection_name\": collection_name\n",
    "    }\n",
    "        \n",
    "    try:\n",
    "        docs = []\n",
    "        # Use HTTPS and construct URL without port if it's a standard HTTPS service\n",
    "        if BASE_URL.startswith('https://') or BASE_URL.startswith('http://'):\n",
    "            url_search = f\"{BASE_URL}/v1/search\"\n",
    "        else:\n",
    "            url_search = f\"http://{BASE_URL}/v1/search\"\n",
    "        with requests.post(url_search, json=data_search, timeout=TIMEOUT, verify=False) as req:\n",
    "            req.raise_for_status()\n",
    "            context_doc = req.json()\n",
    "            for doc in context_doc.get(\"results\", []):\n",
    "                if doc.get(\"document_name\", None):\n",
    "                    docs.append(doc.get(\"document_name\")+\"_\"+str(doc.get(\"metadata\").get(\"content_metadata\").get(\"page_number\")))\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get response from /search endpoint of rag-server. Error details:\"\n",
    "                    f\"{e}. Refer to rag-server logs for details.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d20cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_dict(eval_dataset, BASE_URL, collection_name, top_k):\n",
    "    \"\"\"Create a evaulation dictionary with generated response\"\"\"\n",
    "    eval_data = []\n",
    "    total_questions = len(eval_dataset)  # Total number of queries to process\n",
    "        \n",
    "    for d in eval_dataset:\n",
    "        try:\n",
    "            retrieved_docs = get_retrieved_documents(d.get('question'), BASE_URL, collection_name, top_k)\n",
    "            result = {\n",
    "                'id': d.get('id'),\n",
    "                'question': d.get('question'),\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "            }\n",
    "            eval_data.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {d.get('question')}: {e}\")\n",
    "            error_count += 1\n",
    "            eval_data.append(None)\n",
    "            \n",
    "    return eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_metric(dataset_name, eval_data, granularity):\n",
    "    \"\"\"\n",
    "    Calculate recall metrics for RAG (Retrieval-Augmented Generation) evaluation at multiple top-k levels.\n",
    "    \n",
    "    This function evaluates how well the retrieval system performs by measuring the proportion \n",
    "    of relevant documents that are successfully retrieved at different top-k cutoffs (1, 3, 5, 10).\n",
    "    It supports both page-level and document-level granularity to accommodate different evaluation needs.\n",
    "    \n",
    "    The recall metric is calculated as:\n",
    "        Recall@k = (Number of relevant documents in top-k retrieved) / (Total number of relevant documents)\n",
    "    \"\"\"\n",
    "    # Load ground truth data containing relevant documents and pages for each query\n",
    "    gt_files_pages = {}\n",
    "    \n",
    "    # Validate ground truth file exists\n",
    "    if not os.path.exists(GT_FILENAME_PAGE_DATA):\n",
    "        print(f\"Error: Ground truth file '{GT_FILENAME_PAGE_DATA}' not found. Skipping recall calculation.\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Load ground truth data\n",
    "    try:\n",
    "        with open(GT_FILENAME_PAGE_DATA, 'r', encoding='utf-8') as f:\n",
    "            gt_files_pages = json.load(f)\n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        print(f\"Error loading ground truth file: {e}. Skipping recall calculation.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Validate ground truth data is not empty\n",
    "    if not gt_files_pages:\n",
    "        print(f\"Error: Ground truth file contains no data for dataset '{dataset_name}'. Skipping recall calculation.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Initialize recall metric storage for different top-k values\n",
    "    recall_metrics = {\n",
    "        1: [],   # Recall@1 scores\n",
    "        3: [],   # Recall@3 scores  \n",
    "        5: [],   # Recall@5 scores\n",
    "        10: []   # Recall@10 scores\n",
    "    }\n",
    "    \n",
    "    # Process each evaluation sample to calculate recall metrics\n",
    "    for idx, sample in enumerate(eval_data):\n",
    "        # Validate that ground truth exists for this sample index\n",
    "        if idx >= len(gt_files_pages):\n",
    "            print(f\"Warning: No ground truth found for sample index {idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Skip samples with no retrieved documents\n",
    "        if not sample.get(\"retrieved_docs\") or len(sample[\"retrieved_docs\"]) == 0:\n",
    "            print(f\"Warning: No retrieved documents for question: '{sample.get('question', 'Unknown')}'\")\n",
    "            continue\n",
    "\n",
    "        # Extract top-k retrieved documents for different recall levels\n",
    "        retrieved_docs_by_k = {\n",
    "            1: sample[\"retrieved_docs\"][:1],\n",
    "            3: sample[\"retrieved_docs\"][:3], \n",
    "            5: sample[\"retrieved_docs\"][:5],\n",
    "            10: sample[\"retrieved_docs\"][:10]\n",
    "        }\n",
    "\n",
    "        # Determine evaluation granularity and extract relevant documents from ground truth\n",
    "        gt_contexts = gt_files_pages[idx][\"contexts\"]\n",
    "        page_number_available = bool(gt_contexts[0].get(\"page\", \"\").strip())\n",
    "        \n",
    "        if page_number_available and granularity == \"page\":\n",
    "            # Page-level evaluation: include page numbers in document identifiers\n",
    "            relevant_docs = [f\"{context['filename']}_{context['page']}\" for context in gt_contexts]\n",
    "        else:\n",
    "            # Document-level evaluation: strip page numbers for comparison\n",
    "            # This handles both cases: when page numbers are unavailable OR when granularity=\"document\"\n",
    "            relevant_docs = [context[\"filename\"] for context in gt_contexts]\n",
    "            \n",
    "            # Strip page numbers from retrieved documents to match granularity\n",
    "            for k in retrieved_docs_by_k:\n",
    "                retrieved_docs_by_k[k] = [doc.rsplit(\"_\", 1)[0] for doc in retrieved_docs_by_k[k]]\n",
    "        \n",
    "        # Calculate recall@k for each k value using set intersection for efficiency\n",
    "        for k in recall_metrics.keys():\n",
    "            retrieved_set = set(retrieved_docs_by_k[k])\n",
    "            relevant_set = set(relevant_docs)\n",
    "            \n",
    "            # Count relevant documents that were successfully retrieved\n",
    "            num_relevant_retrieved = len(retrieved_set.intersection(relevant_set))\n",
    "            \n",
    "            # Calculate recall as fraction of relevant documents retrieved\n",
    "            recall_score = num_relevant_retrieved / len(relevant_docs) if relevant_docs else 0.0\n",
    "            recall_metrics[k].append(recall_score)\n",
    "\n",
    "    # Return page availability flag and recall metrics for all k values\n",
    "    return (page_number_available, \n",
    "            recall_metrics[1], \n",
    "            recall_metrics[3], \n",
    "            recall_metrics[5], \n",
    "            recall_metrics[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_result(dataset_name, eval_data, all_results, BASE_URL, collection_name, top_k):\n",
    "    eval_dict = create_eval_dict(eval_data, BASE_URL, collection_name, top_k)\n",
    "    # Calculate page-level recall metrics (includes page numbers in matching)\n",
    "    page_number_available, page_level_recall_1, page_level_recall_3, page_level_recall_5, page_level_recall_10 = get_recall_metric(dataset_name, eval_dict, \"page\")\n",
    "    \n",
    "    # Calculate document-level recall metrics (ignores page numbers in matching)\n",
    "    _, document_level_recall_1, document_level_recall_3, document_level_recall_5, document_level_recall_10 = get_recall_metric(dataset_name, eval_dict, \"document\")\n",
    "\n",
    "    # Add recall@1 metrics if top_k supports it\n",
    "    if int(top_k) >= 1:\n",
    "        if page_number_available and page_level_recall_1:\n",
    "            all_results[\"page_level_recall_1\"] = page_level_recall_1\n",
    "        if document_level_recall_1:\n",
    "            all_results[\"document_level_recall_1\"] = document_level_recall_1\n",
    "    \n",
    "    # Add recall@3 metrics if top_k supports it\n",
    "    if int(top_k) >= 3:\n",
    "        if page_number_available and page_level_recall_3:\n",
    "            all_results[\"page_level_recall_3\"] = page_level_recall_3\n",
    "        if document_level_recall_3:\n",
    "            all_results[\"document_level_recall_3\"] = document_level_recall_3\n",
    "    \n",
    "    # Add recall@5 metrics if top_k supports it        \n",
    "    if int(top_k) >= 5:\n",
    "        if page_number_available and page_level_recall_5:\n",
    "            all_results[\"page_level_recall_5\"] = page_level_recall_5\n",
    "        if document_level_recall_5:\n",
    "            all_results[\"document_level_recall_5\"] = document_level_recall_5\n",
    "    \n",
    "    # Add recall@10 metrics if top_k supports it\n",
    "    if int(top_k) >= 10:\n",
    "        if page_number_available and page_level_recall_10:\n",
    "            all_results[\"page_level_recall_10\"] = page_level_recall_10\n",
    "        if document_level_recall_10:\n",
    "            all_results[\"document_level_recall_10\"] = document_level_recall_10\n",
    "\n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264553d",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation\n",
    "\n",
    "Finally, let's kick off our evaluation pipeline for Recall and print out the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "all_results = {}\n",
    "\n",
    "eval_data_path = os.path.join(DATASET_BASE_DIR, EVAL_DATA)\n",
    "eval_data = get_eval_data(dataset_path=eval_data_path)\n",
    "\n",
    "all_result = evaluate_result(dataset_name, eval_data, all_results, BASE_URL, collection_name, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Recall Metrics (document-level)\")\n",
    "if int(top_k) >= 1 and 'document_level_recall_1' in all_result:\n",
    "    print(f\"     -Recall@1:                      {all_result['document_level_recall_1'].mean()}\")\n",
    "if int(top_k) >= 3 and 'document_level_recall_3' in all_result:\n",
    "    print(f\"     -Recall@3:                      {all_result['document_level_recall_3'].mean()}\")\n",
    "# The following lines were added to support recall@5 and recall@10\n",
    "if int(top_k) >= 5 and 'document_level_recall_5' in all_result:\n",
    "    print(f\"     -Recall@5:                      {all_result['document_level_recall_5'].mean()}\")\n",
    "if int(top_k) >= 10 and 'document_level_recall_10' in all_result:\n",
    "    print(f\"     -Recall@10:                     {all_result['document_level_recall_10'].mean()}\")\n",
    "# The following lines were added to support page-level recall metrics\n",
    "if 'page_level_recall_1' in all_result:\n",
    "    print(\"- Recall Metrics (page-level)\")\n",
    "    if int(top_k) >= 1 and 'page_level_recall_1' in all_result:\n",
    "        print(f\"     -Recall@1:                      {all_result['page_level_recall_1'].mean()}\")\n",
    "    if int(top_k) >= 3 and 'page_level_recall_3' in all_result:\n",
    "        print(f\"     -Recall@3:                      {all_result['page_level_recall_3'].mean()}\")\n",
    "    if int(top_k) >= 5 and 'page_level_recall_5' in all_result:\n",
    "        print(f\"     -Recall@5:                      {all_result['page_level_recall_5'].mean()}\")\n",
    "    if int(top_k) >= 10 and 'page_level_recall_10' in all_result:\n",
    "        print(f\"     -Recall@10:                     {all_result['page_level_recall_10'].mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
